#!/usr/bin/env python3
import datetime
import os
import time
import torch
import model
import math
import dataset
import pickle
import numpy as np
from torch.autograd import Variable
import tensorflow as tf
import torch.utils.data as Data
import cv2
from six.moves import xrange
from transformations import quaternion_from_matrix

adjacency_num = 8
min_kp_num = 500
margin = 0.05
def evaluate_R_t(R_gt, t_gt, R, t, q_gt=None):

    # from Utils.transformations import quaternion_from_matrix

    t = t.flatten()
    t_gt = t_gt.flatten()

    eps = 1e-15

    if q_gt is None:
        q_gt = quaternion_from_matrix(R_gt)
    q = quaternion_from_matrix(R)
    q = q / (np.linalg.norm(q) + eps)
    q_gt = q_gt / (np.linalg.norm(q_gt) + eps)
    loss_q = np.maximum(eps, (1.0 - np.sum(q * q_gt)**2))
    err_q = np.arccos(1 - 2 * loss_q)

    # dR = np.dot(R, R_gt.T)
    # dt = t - np.dot(dR, t_gt)
    # dR = np.dot(R, R_gt.T)
    # dt = t - t_gt
    t = t / (np.linalg.norm(t) + eps)
    t_gt = t_gt / (np.linalg.norm(t_gt) + eps)
    loss_t = np.maximum(eps, (1.0 - np.sum(t * t_gt)**2))
    err_t = np.arccos(np.sqrt(1 - loss_t))

    if np.sum(np.isnan(err_q)) or np.sum(np.isnan(err_t)):
        # This should never happen! Debug here
        import IPython
        IPython.embed()

    return err_q, err_t

def eval_nondecompose(p1s, p2s, E_hat, dR, dt, scores):

    # Use only the top 10% in terms of score to decompose, we can probably
    # implement a better way of doing this, but this should be just fine.
    num_top = len(scores) // 10
    num_top = max(1, num_top)
    th = np.sort(scores)[::-1][num_top]
    mask = scores >= th

    p1s_good = p1s[mask]
    p2s_good = p2s[mask]

    # Match types
    E_hat = E_hat.reshape(3, 3).astype(p1s.dtype)

    if p1s_good.shape[0] >= 5:
        # Get the best E just in case we get multipl E from findEssentialMat
        num_inlier, R, t, mask_new = cv2.recoverPose(
            E_hat, p1s_good, p2s_good)
        try:
            err_q, err_t = evaluate_R_t(dR, dt, R, t)
        except:
            print("Failed in evaluation")
            print(R)
            print(t)
            err_q = np.pi
            err_t = np.pi / 2
    else:
        err_q = np.pi
        err_t = np.pi / 2

    loss_q = np.sqrt(0.5 * (1 - np.cos(err_q)))
    loss_t = np.sqrt(1.0 - np.cos(err_t)**2)

    # Change mask type
    mask = mask.flatten().astype(bool)

    mask_updated = mask.copy()
    if mask_new is not None:
        # Change mask type
        mask_new = mask_new.flatten().astype(bool)
        mask_updated[mask] = mask_new
    if err_q == 0:
        print(err_t)
    return err_q, err_t, loss_q, loss_t, np.sum(num_inlier), mask_updated

def estimate_E(input, weight):
    E = []
    ones = torch.ones(input.size(2))
    data = []
    data.append(torch.unsqueeze(input[:, 0, :, 0], -1))  # u1
    data.append(torch.unsqueeze(input[:, 0, :, 1], -1)) # v1
    data.append(torch.unsqueeze(input[:, 0, :, 2], -1))  # u2
    data.append(torch.unsqueeze(input[:, 0, :, 3], -1))
    data.append(torch.unsqueeze(torch.ones(input.size(0),input.size(2)).cuda(), -1))
    X = torch.cat((data[2] * data[0], data[2] * data[1], data[2], data[3] * data[0], data[3] * data[1],
                       data[3], data[0], data[1], data[4]), -1)

    W = torch.stack([torch.diag(weight[i]) for i in range(self.batch_size)])
    M = torch.bmm(X.transpose(1, 2), W)
    M = torch.bmm(M, X)
    svd = torch.stack([torch.svd(M[i])[2][:, 8] for i in range(self.batch_size)])
    E = (svd / (torch.norm(svd, 2, dim=-1, keepdim=True) + 1e-6))
    return E
    
def local_consistency(xs_initial, affine):  #16*1*2000*4 16*2000*18
    x = xs_initial[:, 0, :, 0:2]
    y = xs_initial[:, 0, :, 2:4]
    affine = affine.view(-1, 18)    
    affine_x = torch.stack([torch.inverse(affine[i][:9].view(3, 3)) for i in range (affine.shape[0])])
    affine_y = affine[:, 9:].view(-1, 3, 3)
        
    H = torch.bmm(affine_y, affine_x).view(xs_initial.size(0), xs_initial.size(2), 3, 3)
    
    ones = torch.ones(x.size(0), x.size(1), 1)
    x = torch.cat((x, ones), dim=-1)
    y = torch.cat((y, ones), dim=-1)
    index = []
    for i in range(x.size(0)):
        x_repeat = x[i].t().repeat(x[i].size(0), 1, 1)
        y_prj = torch.bmm(H[i], x_repeat)
        y_prj = (y_prj / y_prj[:, 2, :].unsqueeze(1))[:, 0:2, :]
        
        prj_dis = torch.stack([torch.sum(torch.pow((y_prj[idx] - y_prj[idx, :, idx].unsqueeze(-1)), 2), dim=0) for idx in range(x[i].size(0))])
        prj_dis = prj_dis + prj_dis.t()
        index.append(torch.sort(prj_dis, dim=1, descending=False)[1][:, :8]) 
    return torch.stack(index)
    
def local_score(xs_initial, affine):  #16*1*2000*4 16*2000*18
    x = xs_initial[:, 0, :, 0:2]
    y = xs_initial[:, 0, :, 2:4]
    affine = affine.view(-1, 18)    
    affine_x = torch.stack([torch.inverse(affine[i][:9].view(3, 3)) for i in range (affine.shape[0])])
    affine_y = affine[:, 9:].view(-1, 3, 3)
        
    H = torch.bmm(affine_y, affine_x).view(xs_initial.size(0), xs_initial.size(2), 3, 3)
    
    ones = torch.ones(x.size(0), x.size(1), 1)
    x = torch.cat((x, ones), dim=-1)
    y = torch.cat((y, ones), dim=-1)
    score = []
    for i in range(x.size(0)):
        x_repeat = x[i].t().repeat(x[i].size(0), 1, 1)
        y_prj = torch.bmm(H[i], x_repeat)
        y_prj = (y_prj / y_prj[:, 2, :].unsqueeze(1))[:, 0:2, :]
        
        prj_dis = torch.stack([torch.sum(torch.pow((y_prj[idx] - y_prj[idx, :, idx].unsqueeze(-1)), 2), dim=0) for idx in range(x[i].size(0))])
        prj_dis = prj_dis + prj_dis.t()
        score.append(torch.sort(prj_dis, dim=1, descending=False)[0][:, :8]) 
    score = torch.stack(score)
    score = (-margin * score).exp()
    
    return score  
  
def test_process(mode, save_file_cur, res_dir, model_name, config, score_feature=False):
    
    if not os.path.exists(os.path.join(res_dir, mode + "-score")):
        os.makedirs(os.path.join(res_dir, mode + "-score"))
        
    valid_score_path = os.path.join(res_dir, mode + "-score") 
    data = dataset.load_data(config, mode)
     
    xs = data["xs"]
    ys = data["ys"]
    Rs = data["Rs"]
    ts = data["ts"]    
    if score_feature == True:
        #xs_initial = data["xs_initial"] 
        #affine = data["affine"] 
        index = data["index"] 
    # Validation
    num_sample = len(xs)

    pointnet = torch.load(save_file_cur + model_name).cuda()
    pointnet.eval()

    test_list = []
    test_list += ["ours"]
    eval_res = {}
    measure_list = ["err_q", "err_t", "num"]
    P = []
    R = []
    F = []
    for measure in measure_list:
        eval_res[measure] = {}
        for _test in test_list:
            eval_res[measure][_test] = np.zeros(num_sample)
            
    for idx_cur in xrange(num_sample):
        # Use minimum kp in batch to construct the batch
        
        xs_b = torch.from_numpy(np.array(
                xs[idx_cur][:, :, :]).reshape(1, 1, -1, 4).astype(np.float32)).cuda()
        ys_b = torch.from_numpy(np.array(
                ys[idx_cur][:, :]).reshape(1, -1, 2).astype(np.float32))
        Rs_b = torch.from_numpy(np.array(Rs[idx_cur]).reshape(1, 9).astype(np.float32))
        ts_b = torch.from_numpy(np.array(ts[idx_cur]).reshape(1, 3).astype(np.float32))
        
        if score_feature == True:
            xs_ini_b = torch.from_numpy(np.array(xs_initial[idx_cur]).reshape(1, 1, -1, 4).astype(np.float32))
            affine_b = torch.from_numpy(np.array(affine[idx_cur]).astype(np.float32))
            if model_name == '/LPMNet_state.pth' or model_name == '/LPMNet_best_state.pth':
                score_b = local_score(xs_ini_b, affine_b).cuda()
                score_b = score_b.view(1, -1)
                score_b = (score_b - torch.mean(score_b, dim=-1).unsqueeze(-1)) / torch.std(score_b, dim=-1).unsqueeze(-1)
                score_b = score_b.view(1, adjacency_num, -1, 1)
                output, weight = pointnet(score_b)
            else:
                index_b = torch.from_numpy(index[idx_cur]).unsqueeze(0).cuda()
                output, weight = pointnet(xs_b, index_b)
        else:                    
            output, weight = pointnet(xs_b)
        
        label = (ys_b[:, :, 0] < config.obj_geod_th).type(torch.FloatTensor)
        mask = (weight > 0).type(torch.FloatTensor)
        p = torch.sum(mask * label) / torch.sum(mask)
        if math.isnan(p):
            p = torch.Tensor([0])
        r = torch.sum(mask * label) / torch.sum(label) 
        if math.isnan(r):
            r = torch.Tensor([0])
        f = 2 * p * r / (p + r)
        if math.isnan(f):
            f = torch.Tensor([0]) 
        P.append(p.cpu().numpy())
        R.append(r.cpu().numpy())
        F.append(f.cpu().numpy())
    
    p_ = np.expand_dims(np.mean(np.array(P)), axis=0)
    r_ = np.expand_dims(np.mean(np.array(R)), axis=0)
    f_ = np.expand_dims(np.mean(np.array(F)), axis=0)
    
    np.savetxt(os.path.join(res_dir, mode, "Precision.txt"), p_)
    np.savetxt(os.path.join(res_dir, mode, "Recall.txt"), r_)
    np.savetxt(os.path.join(res_dir, mode, "F-measure.txt"), f_)
    return f_
    '''
        E = estimate_E(xs_b, weight)
        _xs = xs_b.numpy()
        _ys = ys_b.numpy()
        _dR = Rs_b.numpy()
        _dt = ts_b.numpy()

        e_hat_out = E.cpu().numpy().flatten()
        y_hat_out = output.cpu().numpy().flatten()
        
        if len(y_hat_out) != _xs.shape[2]:
            y_hat_out = np.ones(_xs.shape[2])
        # Eval decompose for all pairs
        _xs = _xs.reshape(-1, 4)
        # x coordinates
        _x1 = _xs[:, :2].astype(np.float64)
        _x2 = _xs[:, 2:].astype(np.float64)
        # current validity from network
        _valid = y_hat_out.flatten()
        # choose top ones (get validity threshold)
        _valid_th = np.sort(_valid)[::-1][config.obj_top_k]
        print('_valid_th:', _valid_th)
        # For every things to test
        _use_prob = True
        _eval_func = "non-decompose"
        _mask_before = _valid >= max(0, _valid_th)
        _method = None
        _probs = None
        _weighted = False

        _err_q, _err_t, _, _, _num_inlier, _ = eval_nondecompose(
            _x1, _x2, e_hat_out, _dR, _dt, y_hat_out)
        _mask_after = _mask_before
        # Load them in list
        eval_res["err_q"][_test][idx_cur] = _err_q
        eval_res["err_t"][_test][idx_cur] = _err_t
        eval_res["num"][_test][idx_cur] = _num_inlier

    summaries = []
    ret_val = 0
    for _tag in test_list:
        for _sub_tag in measure_list:
            summaries.append(
                tf.Summary.Value(
                    tag="ErrorComputation/" + _tag,
                    simple_value=np.mean(eval_res[_sub_tag][_tag])
                )
            )
            # For mean error
            if not os.path.exists(os.path.join(res_dir, mode)):
                os.makedirs(os.path.join(res_dir, mode))
                
            ofn = os.path.join(
                res_dir, mode, "mean_{}_{}.txt".format(_sub_tag, _tag))
            with open(ofn, "w") as ofp:
                ofp.write("{}\n".format(
                    np.mean(eval_res[_sub_tag][_tag])))

        ths = np.arange(7) * 5
        cur_err_q = np.array(eval_res["err_q"][_tag]) * 180.0 / np.pi
        cur_err_t = np.array(eval_res["err_t"][_tag]) * 180.0 / np.pi
        # Get histogram
        q_acc_hist, _ = np.histogram(cur_err_q, ths)
        t_acc_hist, _ = np.histogram(cur_err_t, ths)
        qt_acc_hist, _ = np.histogram(np.maximum(cur_err_q, cur_err_t), ths)
        num_pair = float(len(cur_err_q))
        q_acc_hist = q_acc_hist.astype(float) / num_pair
        t_acc_hist = t_acc_hist.astype(float) / num_pair
        qt_acc_hist = qt_acc_hist.astype(float) / num_pair
        q_acc = np.cumsum(q_acc_hist)
        t_acc = np.cumsum(t_acc_hist)
        qt_acc = np.cumsum(qt_acc_hist)
        # Store return val
        if _tag == "ours":
            ret_val = np.mean(qt_acc[:4])  # 1 == 5
        for _idx_th in xrange(1, len(ths)):
            summaries += [
                tf.Summary.Value(
                    tag="ErrorComputation/acc_q_auc{}_{}".format(
                        ths[_idx_th], _tag),
                    simple_value=np.mean(q_acc[:_idx_th]),
                )
            ]
            summaries += [
                tf.Summary.Value(
                    tag="ErrorComputation/acc_t_auc{}_{}".format(
                        ths[_idx_th], _tag),
                    simple_value=np.mean(t_acc[:_idx_th]),
                )
            ]
            summaries += [
                tf.Summary.Value(
                    tag="ErrorComputation/acc_qt_auc{}_{}".format(
                        ths[_idx_th], _tag),
                    simple_value=np.mean(qt_acc[:_idx_th]),
                )
            ]
            # for q_auc
            ofn = os.path.join(
                res_dir, mode,
                "acc_q_auc{}_{}.txt".format(ths[_idx_th], _tag))
            with open(ofn, "w") as ofp:
                ofp.write("{}\n".format(np.mean(q_acc[:_idx_th])))
            # for qt_auc
            ofn = os.path.join(
                res_dir, mode,
                "acc_t_auc{}_{}.txt".format(ths[_idx_th], _tag))
            with open(ofn, "w") as ofp:
                ofp.write("{}\n".format(np.mean(t_acc[:_idx_th])))
            # for qt_auc
            ofn = os.path.join(
                res_dir, mode,
                "acc_qt_auc{}_{}.txt".format(ths[_idx_th], _tag))
            with open(ofn, "w") as ofp:
                ofp.write("{}\n".format(np.mean(qt_acc[:_idx_th])))

    if mode == "test":
        print("[{}] {}: End testing".format(
            config.data_tr, time.asctime()))

    # Return qt_auc20 of ours
    return ret_val
    '''
